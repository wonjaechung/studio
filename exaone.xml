This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where security check has been disabled.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Security check has been disabled - content may contain sensitive information
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
LICENSE
README.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="LICENSE">
EXAONE AI Model License Agreement 1.1 - NC

This License Agreement (‚ÄúAgreement‚Äù) is entered into between you (‚ÄúLicensee‚Äù) and LG Management Development 
Institute Co., Ltd. (‚ÄúLicensor‚Äù), governing the use of the EXAONE AI Model (‚ÄúModel‚Äù). By downloading, 
installing, copying, or using the Model, you agree to comply with and be bound by the terms of this Agreement.
If you do not agree to all the terms, you must not download, install, copy, or use the Model. This Agreement 
constitutes a binding legal agreement between the Licensee and Licensor.

1. Definitions
    1.1 Model: The artificial intelligence model provided by Licensor, which includes any software, 
    algorithms, machine learning models, or related components supplied by Licensor. This definition extends 
    to encompass all updates, enhancements, improvements, bug fixes, patches, or other modifications that may 
    be provided by Licensor from time to time, whether automatically or manually implemented.
    1.2 Derivatives: Any modifications, alterations, enhancements, improvements, adaptations, or derivative 
    works of the Model created by Licensee or any third party. This includes changes made to the Model's 
    architecture, parameters, data processing methods, or any other aspect of the Model that results in a 
    modification of its functionality or output.
    1.3 Output: Any data, results, content, predictions, analyses, insights, or other materials generated by 
    the Model or Derivatives, regardless of whether they are in their original form or have been further 
    processed or modified by the Licensee. This includes, but is not limited to, textual or numerical produced 
    directly or indirectly through the use of the Model.
    1.4 Licensor: LG Management Development Institute Co., Ltd., the owner, developer, and provider of the 
    EXAONE AI Model. The Licensor holds all rights, title, and interest in the Model and is responsible for 
    granting licenses to use the Model under the terms specified in this Agreement.
    1.5 Licensee: The individual, organization, corporation, academic institution, government agency, or other 
    entity using or intending to use the Model under the terms and conditions of this Agreement. The Licensee 
    is responsible for ensuring compliance with the Agreement by all authorized users who access or utilize 
    the Model on behalf of the Licensee.

2. License Grant
    2.1 Grant of License: Subject to the terms and conditions outlined in this Agreement, the Licensor hereby 
    grants the Licensee a limited, non-exclusive, non-transferable, worldwide, and revocable license to:
        a. Access, download, install, and use the Model solely for research purposes. This includes 
        evaluation, testing, academic research, experimentation, and participation in competitions, provided 
        that such participation is in a non-commercial context. Notwithstanding Section 3.1, the Licensee may 
        only provide the Model or Derivatives for a competition if no commercial license is granted to the 
        competition organizer or any third party.
        b. Publicly disclose research results and findings derived from the use of the Model or Derivatives, 
        including publishing papers or presentations.
        c. Modify the Model and create Derivatives based on the Model, provided that such modifications and 
        Derivatives are used exclusively for research purposes. The Licensee may conduct experiments, perform 
        analyses, and apply custom modifications to the Model to explore its capabilities and performance 
        under various scenarios. If the Model is modified, the modified Model must include ‚ÄúEXAONE‚Äù at the 
        beginning of its name.
        d. Distribute the Model and Derivatives in each case with a copy of this Agreement.
    2.2 Scope of License: The license granted herein does not authorize the Licensee to use the Model for any 
    purpose not explicitly permitted under this Agreement. Any use beyond the scope of this license, including 
    any commercial application or external distribution, is strictly prohibited unless explicitly agreed upon 
    in writing by the Licensor.

3. Restrictions
    3.1 Commercial Use: The Licensee is expressly prohibited from using the Model, Derivatives, or Output for 
    any commercial purposes, including but not limited to, developing or deploying products, services, or 
    applications that generate revenue, whether directly or indirectly. Any commercial exploitation of the 
    Model or its derivatives requires a separate commercial license agreement with the Licensor. Furthermore, 
    the Licensee shall not use the Model, Derivatives or Output to develop or improve other models.
    3.2 Reverse Engineering: The Licensee shall not decompile, disassemble, reverse engineer, or attempt to 
    derive the source code, underlying ideas, algorithms, or structure of the Model, except to the extent that 
    such activities are expressly permitted by applicable law. Any attempt to bypass or circumvent 
    technological protection measures applied to the Model is strictly prohibited.
    3.3 Unlawful Use: The Licensee shall not use the Model and Derivatives for any illegal, fraudulent, or 
    unauthorized activities, nor for any purpose that violates applicable laws or regulations. This includes 
    but is not limited to the creation, distribution, or dissemination of malicious, deceptive, or unlawful 
    content.
    3.4 Ethical Use: The Licensee shall ensure that the Model or Derivatives is used in an ethical and 
    responsible manner, adhering to the following guidelines:
        a. The Model and Derivatives shall not be used to generate, propagate, or amplify false, misleading, 
        or harmful information, including fake news, misinformation, or disinformation.
        b. The Model and Derivatives shall not be employed to create, distribute, or promote content that is 
        discriminatory, harassing, defamatory, abusive, or otherwise offensive to individuals or groups based 
        on race, gender, sexual orientation, religion, nationality, or other protected characteristics.
        c. The Model and Derivatives shall not infringe on the rights of others, including intellectual 
        property rights, privacy rights, or any other rights recognized by law. The Licensee shall obtain all 
        necessary permissions and consents before using the Model and Derivatives in a manner that may impact 
        the rights of third parties.
        d. The Model and Derivatives shall not be used in a way that causes harm, whether physical, mental, 
        emotional, or financial, to individuals, organizations, or communities. The Licensee shall take all 
        reasonable measures to prevent misuse or abuse of the Model and Derivatives that could result in harm 
        or injury.

4. Ownership
    4.1 Intellectual Property: All rights, title, and interest in and to the Model, including any 
    modifications, Derivatives, and associated documentation, are and shall remain the exclusive property of 
    the Licensor. The Licensee acknowledges that this Agreement does not transfer any ownership rights to the 
    Licensee. All trademarks, service marks, and logos associated with the Model are the property of the 
    Licensor.
    4.2 Output: All rights, title, and interest in and to the Output generated by the Model and Derivatives 
    whether in its original form or modified, are and shall remain the exclusive property of the Licensor.
    Licensee may use, modify, and distribute the Output and its derivatives for research purpose. The Licensee 
    shall not claim ownership of the Output except as expressly provided in this Agreement. The Licensee may 
    use the Output solely for the purposes permitted under this Agreement and shall not exploit the Output for 
    unauthorized or commercial purposes.
    4.3 Attribution: In any publication or presentation of results obtained using the Model, the Licensee 
    shall provide appropriate attribution to the Licensor, citing the Model's name and version, along with any 
    relevant documentation or references specified by the Licensor.

5. No Warranty
    5.1 ‚ÄúAs-Is‚Äù Basis: The Model, Derivatives, and Output are provided on an ‚Äúas-is‚Äù and ‚Äúas-available‚Äù basis, 
    without any warranties or representations of any kind, whether express, implied, or statutory. The 
    Licensor disclaims all warranties, including but not limited to, implied warranties of merchantability, 
    fitness for a particular purpose, accuracy, reliability, non-infringement, or any warranty arising from 
    the course of dealing or usage of trade.
    5.2 Performance and Reliability: The Licensor does not warrant or guarantee that the Model, Derivatives or 
    Output will meet the Licensee‚Äôs requirements, that the operation of the Model, Derivatives or Output will 
    be uninterrupted or error-free, or that defects in the Model will be corrected. The Licensee acknowledges 
    that the use of the Model, Derivatives or Output is at its own risk and that the Model, Derivatives or 
    Output may contain bugs, errors, or other limitations.
    5.3 No Endorsement: The Licensor does not endorse, approve, or certify any results, conclusions, or 
    recommendations derived from the use of the Model. The Licensee is solely responsible for evaluating the 
    accuracy, reliability, and suitability of the Model for its intended purposes.

6. Limitation of Liability
    6.1 No Liability for Damages: To the fullest extent permitted by applicable law, in no event shall the 
    Licensor be liable for any special, incidental, indirect, consequential, exemplary, or punitive damages, 
    including but not limited to, damages for loss of business profits, business interruption, loss of 
    business information, loss of data, or any other pecuniary or non-pecuniary loss arising out of or in 
    connection with the use or inability to use the Model, Derivatives or any Output, even if the Licensor has 
    been advised of the possibility of such damages.
    6.2 Indemnification: The Licensee agrees to indemnify, defend, and hold harmless the Licensor, its 
    affiliates, officers, directors, employees, and agents from and against any claims, liabilities, damages, 
    losses, costs, or expenses (including reasonable attorneys' fees) arising out of or related to the 
    Licensee's use of the Model, any Derivatives, or any Output, including any violation of this Agreement or 
    applicable laws.

7. Termination
    7.1 Termination by Licensor: The Licensor reserves the right to terminate this Agreement and revoke the 
    Licensee‚Äôs rights to use the Model at any time, with or without cause, and without prior notice if the 
    Licensee breaches any of the terms or conditions of this Agreement. Termination shall be effective 
    immediately upon notice.
    7.2 Effect of Termination: Upon termination of this Agreement, the Licensee must immediately cease all use 
    of the Model, Derivatives, and Output and destroy all copies of the Model, Derivatives, and Output in its 
    possession or control, including any backup or archival copies. The Licensee shall certify in writing to 
    the Licensor that such destruction has been completed.
    7.3 Survival: The provisions of this Agreement that by their nature should survive termination, including 
    but not limited to, Sections 4 (Ownership), 5 (No Warranty), 6 (Limitation of Liability), and this Section 
    7 (Termination), shall continue to apply after termination.

8. Governing Law
    8.1 Governing Law: This Agreement shall be governed by and construed in accordance with the laws of the 
    Republic of Korea, without regard to its conflict of laws principles.
    8.2 Arbitration: Any disputes, controversies, or claims arising out of or relating to this Agreement, 
    including its existence, validity, interpretation, performance, breach, or termination, shall be referred 
    to and finally resolved by arbitration administered by the Korean Commercial Arbitration Board (KCAB) in 
    accordance with the International Arbitration Rules of the Korean Commercial Arbitration Board in force at 
    the time of the commencement of the arbitration. The seat of arbitration shall be Seoul, Republic of 
    Korea. The tribunal shall consist of one arbitrator. The language of the arbitration shall be English.

9. Alterations
    9.1 Modifications: The Licensor reserves the right to modify or amend this Agreement at any time, in its 
    sole discretion. Any modifications will be effective upon posting the updated Agreement on the Licensor‚Äôs 
    website or through other means of communication. The Licensee is responsible for reviewing the Agreement 
    periodically for changes. Continued use of the Model after any modifications have been made constitutes 
    acceptance of the revised Agreement.
    9.2 Entire Agreement: This Agreement constitutes the entire agreement between the Licensee and Licensor 
    concerning the subject matter hereof and supersedes all prior or contemporaneous oral or written 
    agreements, representations, or understandings. Any terms or conditions of any purchase order or other 
    document submitted by the Licensee in connection with the Model that are in addition to, different from, 
    or inconsistent with the terms and conditions of this Agreement are not binding on the Licensor and are 
    void.

By downloading, installing, or using the EXAONE AI Model, the Licensee acknowledges that it has read, 
understood, and agrees to be bound by the terms and conditions of this Agreement.
</file>

<file path="README.md">
# EXAONE Deep
<br>
<p align="center">
<img src="assets/EXAONE_Symbol+BI_3d.png", width="400", style="margin: 40 auto;">
<br>
<p align="center"> ü§ó <a href="https://huggingface.co/collections/LGAI-EXAONE/exaone-deep-67d119918816ec6efa79a4aa">Hugging Face</a> &nbsp | &nbsp üìù <a href="https://www.lgresearch.ai/news/view?seq=543"> Blog</a> &nbsp | &nbsp üìë <a href="https://arxiv.org/abs/2503.12524"> Documentation </a>
<br>

<br>

## Introduction

We introduce EXAONE Deep, which exhibits superior capabilities in various reasoning tasks including math and coding benchmarks, ranging from 2.4B to 32B parameters developed and released by LG AI Research. Evaluation results show that 1) EXAONE Deep 2.4B outperforms other models of comparable size, 2) EXAONE Deep 7.8B outperforms not only open-weight models of comparable scale but also a proprietary reasoning model OpenAI o1-mini, and 3) EXAONE Deep 32B demonstrates competitive performance against leading open-weight models.

<br>
<p align="center">
<img src="assets/exaone_deep_overall_performance.png", width="90%", style="margin: 40 auto;">
<br>

Our documentation consists of the following sections:

- [Performance](#performance): Experimental results of EXAONE Deep models.
- [Quickstart](#quickstart): A basic guide to using EXAONE Deep models with Transformers.
- [Quantized Models](#quantized-models): An explanation of quantized EXAONE Deep weights in `AWQ` and `GGUF` format.
- [Run Locally](#run-locally): A guide to running EXAONE Deep models locally with `llama.cpp` and `Ollama` frameworks.
- [Deployment](#deployment): A guide to running EXAONE Deep models with `TensorRT-LLM`, `vLLM`, and `SGLang` deployment frameworks.
- [Usage Guideline](#usage-guideline): A guide to utilizing EXAONE Deep models to achieve the expected performance.

<br>

## News

- 2025.03.18: We release the EXAONE Deep, reasoning enhanced language models, including 2.4B, 7.8B, and 32B. Check out the üìë [Documentation](https://arxiv.org/abs/2503.12524)!

<br>

## Performance

Some experimental results are shown below. The full evaluation results can be found in the [Documentation](https://arxiv.org/abs/2503.12524).

<br>

<table>
    <tr>
        <th>Models</th>
        <th>MATH-500 (pass@1)</th>
        <th>AIME 2024 (pass@1 / cons@64)</th>
        <th>AIME 2025 (pass@1 / cons@64)</th>
        <th>CSAT Math 2025 (pass@1)</th>
        <th>GPQA Diamond (pass@1)</th>
        <th>Live Code Bench (pass@1)</th>
    </tr>
    <tr>
        <td>EXAONE Deep 32B</td>
        <td>95.7</td>
        <td>72.1 / <strong>90.0</strong></td>
        <td>65.8 / <strong>80.0</strong></td>
        <td><strong>94.5</strong></td>
        <td>66.1</td>
        <td>59.5</td>
    </tr>
    <tr>
        <td>DeepSeek-R1-Distill-Qwen-32B</td>
        <td>94.3</td>
        <td>72.6 / 83.3</td>
        <td>55.2 / 73.3</td>
        <td>84.1</td>
        <td>62.1</td>
        <td>57.2</td>
    </tr>
    <tr>
        <td>QwQ-32B</td>
        <td>95.5</td>
        <td>79.5 / 86.7</td>
        <td><strong>67.1</strong> / 76.7</td>
        <td>94.4</td>
        <td>63.3</td>
        <td>63.4</td>
    </tr>
    <tr>
        <td>DeepSeek-R1-Distill-Llama-70B</td>
        <td>94.5</td>
        <td>70.0 / 86.7</td>
        <td>53.9 / 66.7</td>
        <td>88.8</td>
        <td>65.2</td>
        <td>57.5</td>
    </tr>
    <tr>
        <td>DeepSeek-R1 (671B)</td>
        <td><strong>97.3</strong></td>
        <td><strong>79.8</strong> / 86.7</td>
        <td>66.8 / <strong>80.0</strong></td>
        <td>89.9</td>
        <td><strong>71.5</strong></td>
        <td><strong>65.9</strong></td>
    </tr>
    <tr>
        <th colspan="7" height="30px"></th>
    </tr>
    <tr>
        <td>EXAONE Deep 7.8B</td>
        <td><strong>94.8</strong></td>
        <td><strong>70.0</strong> / <strong>83.3</strong></td>
        <td><strong>59.6</strong> / <strong>76.7</strong></td>
        <td><strong>89.9</strong></td>
        <td><strong>62.6</strong></td>
        <td><strong>55.2</strong></td>
    </tr>
    <tr>
        <td>DeepSeek-R1-Distill-Qwen-7B</td>
        <td>92.8</td>
        <td>55.5 / <strong>83.3</strong></td>
        <td>38.5 / 56.7</td>
        <td>79.7</td>
        <td>49.1</td>
        <td>37.6</td>
    </tr>
    <tr>
        <td>DeepSeek-R1-Distill-Llama-8B</td>
        <td>89.1</td>
        <td>50.4 / 80.0</td>
        <td>33.6 / 53.3</td>
        <td>74.1</td>
        <td>49.0</td>
        <td>39.6</td>
    </tr>
    <tr>
        <td>OpenAI o1-mini</td>
        <td>90.0</td>
        <td>63.6 / 80.0</td>
        <td>54.8 / 66.7</td>
        <td>84.4</td>
        <td>60.0</td>
        <td>53.8</td>
    </tr>
    <tr>
        <th colspan="7" height="30px"></th>
    </tr>
    <tr>
        <td>EXAONE Deep 2.4B</td>
        <td><strong>92.3</strong></td>
        <td><strong>52.5</strong> / <strong>76.7</strong></td>
        <td><strong>47.9</strong> / <strong>73.3</strong></td>
        <td><strong>79.2</strong></td>
        <td><strong>54.3</strong></td>
        <td><strong>46.6</strong></td>
    </tr>
    <tr>
        <td>DeepSeek-R1-Distill-Qwen-1.5B</td>
        <td>83.9</td>
        <td>28.9 / 52.7</td>
        <td>23.9 / 36.7</td>
        <td>65.6</td>
        <td>33.8</td>
        <td>16.9</td>
    </tr>
</table>

<br>

## Quickstart

- You need to install `transformers>=4.43.1` for the EXAONE Deep models. The latest version is recommended to use.

Here is the example code to show how to use EXAONE Deep models.

> [!Tip]
> In all the examples below, you can use another size model by changing 7.8B to 32B or 2.4B.

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
from threading import Thread

model_name = "LGAI-EXAONE/EXAONE-Deep-7.8B"
streaming = True    # choose the streaming option

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Choose your prompt:
#   Math example (AIME 2024)
prompt = r"""Let $x,y$ and $z$ be positive real numbers that satisfy the following system of equations:
\[\log_2\left({x \over yz}\right) = {1 \over 2}\]\[\log_2\left({y \over xz}\right) = {1 \over 3}\]\[\log_2\left({z \over xy}\right) = {1 \over 4}\]
Then the value of $\left|\log_2(x^4y^3z^2)\right|$ is $\tfrac{m}{n}$ where $m$ and $n$ are relatively prime positive integers. Find $m+n$.

Please reason step by step, and put your final answer within \boxed{}."""
#   Korean MCQA example (CSAT Math 2025)
prompt = r"""Question : $a_1 = 2$Ïù∏ ÏàòÏó¥ $\{a_n\}$Í≥º $b_1 = 2$Ïù∏ Îì±Ï∞®ÏàòÏó¥ $\{b_n\}$Ïù¥ Î™®Îì† ÏûêÏó∞Ïàò $n$Ïóê ÎåÄÌïòÏó¨\[\sum_{k=1}^{n} \frac{a_k}{b_{k+1}} = \frac{1}{2} n^2\]ÏùÑ ÎßåÏ°±ÏãúÌÇ¨ Îïå, $\sum_{k=1}^{5} a_k$Ïùò Í∞íÏùÑ Íµ¨ÌïòÏó¨Îùº.

Options :
A) 120
B) 125
C) 130
D) 135
E) 140
 
Please reason step by step, and you should write the correct option alphabet (A, B, C, D or E) within \\boxed{}."""

messages = [
    {"role": "user", "content": prompt}
]
input_ids = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_tensors="pt"
)

if streaming:
    streamer = TextIteratorStreamer(tokenizer)
    thread = Thread(target=model.generate, kwargs=dict(
        input_ids=input_ids.to("cuda"),
        eos_token_id=tokenizer.eos_token_id,
        max_new_tokens=32768,
        do_sample=True,
        temperature=0.6,
        top_p=0.95,
        streamer=streamer
    ))
    thread.start()

    for text in streamer:
        print(text, end="", flush=True)
else:
    output = model.generate(
        input_ids.to("cuda"),
        eos_token_id=tokenizer.eos_token_id,
        max_new_tokens=32768,
        do_sample=True,
        temperature=0.6,
        top_p=0.95,
    )
    print(tokenizer.decode(output[0]))
```

> [!Important]
> The EXAONE Deep models are trained with an optimized configuration,
> so we recommend following the [Usage Guideline](#usage-guideline) section to achieve optimal performance.

<br>

## Quantized Models

We introduce a series of quantized weights of EXAONE Deep models.

### AWQ

We provide AWQ-quantized weights of EXAONE Deep models, quantized using `AutoAWQ` library. Please refer to the [EXAONE Deep collection](https://huggingface.co/collections/LGAI-EXAONE/exaone-deep-67d119918816ec6efa79a4aa) for pre-quantized weights, and the [AutoAWQ documentation](https://github.com/casper-hansen/AutoAWQ) for more details.

You need to install the latest version of AutoAWQ library (`autoawq>=0.2.8`) to load the AWQ-quantized version of EXAONE Deep models.

```bash
pip install autoawq
```

You can load the model in similar ways to the original models, only changing the model name. It automatically loads with AWQ configuration of the model. Please check the [Quickstart section](#quickstart) above for more details.

### GGUF

We provide weights in `BF16` format and quantized weights in `Q8_0`, `Q6_K`, `Q5_K_M`, `Q4_K_M`, `IQ4_XS`. 

The example below is for the 7.8B model in BF16 format. Please refer to the [EXAONE Deep collection](https://huggingface.co/collections/LGAI-EXAONE/exaone-deep-67d119918816ec6efa79a4aa) to find quantized models. You may need to install `huggingface_hub` to download the GGUF weights.

```bash
# (optional) install huggingface_hub
pip install huggingface_hub

# Download the GGUF weights
huggingface-cli download LGAI-EXAONE/EXAONE-Deep-7.8B-GGUF \
    --include "EXAONE-Deep-7.8B-BF16*.gguf" \
    --local-dir .
```

## Run Locally

For end users, we introduce two ways to run EXAONE Deep models locally. 

> [!Note]
> We highly recommend to use repetition penalty not exceeding 1.0 for better generation quality.

### llama.cpp

You can run EXAONE models with llama.cpp as follows:

1. Install llama.cpp. Please refer to the [llama.cpp repository](https://github.com/ggerganov/llama.cpp) for more details.

2. Download EXAONE Deep model in GGUF format.

```bash
huggingface-cli download LGAI-EXAONE/EXAONE-Deep-7.8B-GGUF \
    --include "EXAONE-Deep-7.8B-BF16*.gguf" \
    --local-dir .
```

3. Run the model with llama.cpp in conversational mode. We set chat-template explicitly to handle reasoning steps properly. 

```bash
llama-cli -m ./EXAONE-Deep-7.8B-BF16.gguf \
    -sys "" \
    -c 32768 \
    --temp 0.6 \
    --top-p 0.95 \
    --jinja \
    --chat-template "{% for message in messages %}{% if loop.first and message['role'] != 'system' %}{{ '[|system|][|endofturn|]\n' }}{% endif %}{% set content = message['content'] %}{% if '</thought>' in content %}{% set content = content.split('</thought>')[-1].lstrip('\\n') %}{% endif %}{{ '[|' + message['role'] + '|]' + content }}{% if not message['role'] == 'user' %}{{ '[|endofturn|]' }}{% endif %}{% if not loop.last %}{{ '\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '\n[|assistant|]<thought>\n' }}{% endif %}"
```

- In case of using EXAONE Deep 32B model with BF16 precision, you may need to download all split files and merge them before running the model.

    ```bash
    # Download all split files
    huggingface-cli download LGAI-EXAONE/EXAONE-Deep-32B-GGUF \
        --include "EXAONE-Deep-32B-BF16*.gguf" \
        --local-dir .

    # Merge all split files
    llama-gguf-split --merge \
        ./EXAONE-Deep-32B-BF16-00001-of-00002.gguf \
        ./EXAONE-Deep-32B-BF16.gguf
    ```

### Ollama

EXAONE Deep models are uploaded to Ollama model library. You can easily use EXAONE models as follows:

1. Install Ollama. Please refer to the [Ollama repository](https://github.com/ollama/ollama) for more details.

2. Run EXAONE Deep model as follows:
```bash
ollama run exaone-deep:7.8b
```

> [!Note]
> In above example, the model `exaone-deep:7.8b` is quantized in `Q4_K_M`. If you would like to know a list of available models, 
> please refer to the [EXAONE Deep Ollama page](https://ollama.com/library/exaone-deep) for more details.

Or, you can create and run EXAONE Deep models with GGUF format for customizing.

1. Install Ollama. Please refer to the [Ollama repository](https://github.com/ollama/ollama) for more details.

2. Download EXAONE Deep model in GGUF format. Please refer to the [GGUF section](#gguf) for more details.

3. Write the `Modelfile` for EXAONE Deep.

```text
# Model path (choose appropriate GGUF weights on your own)
FROM ./EXAONE-Deep-7.8B-BF16.gguf

# Parameter values
PARAMETER stop "[|endofturn|]"
PARAMETER repeat_penalty 1.0
PARAMETER num_ctx 32768
PARAMETER temperature 0.6
PARAMETER top_p 0.95

# Chat template
#   Note: currently there is no feature of removing `<thought></thought>` steps from context 
#   because ollama does not support yet. We will update when according feature is available.
TEMPLATE """{{- range $i, $_ := .Messages }}
{{- $last := eq (len (slice $.Messages $i)) 1 -}}
{{ if eq .Role "system" }}[|system|]{{ .Content }}[|endofturn|]
{{ continue }}
{{ else if eq .Role "user" }}[|user|]{{ .Content }}
{{ else if eq .Role "assistant" }}[|assistant|]{{ .Content }}[|endofturn|]
{{ end }}
{{- if and (ne .Role "assistant") $last }}[|assistant|]<thought>
{{ end }}
{{- end -}}"""

# System prompt
SYSTEM """"""

# License
LICENSE """EXAONE AI Model License Agreement 1.1 - NC """
```

4. Convert the model to Ollama.
```bash
ollama create exaone -f Modelfile
```

3. Run the model with Ollama.
```bash
ollama run exaone
```

### LM-Studio

You can run EXAONE Deep models on your device with LM-Studio.

1. Install LM-Studio. Please refer to the [LM-Studio Page](https://lmstudio.ai/) for more details.

2. Download EXAONE Deep model in GGUF format. You can search and find proper model at `Model Search`.

3. Configure the prompt setting. 
    - Set **"Reasoning Section Parsing"** as `<thought>` and `</thought>`
    - Set **"Template (Jinja)"** as that of EXAONE 3.5.
        ```
        {% for message in messages %}{% if loop.first and message['role'] != 'system' %}{{ '[|system|][|endofturn|]\n' }}{% endif %}{{ '[|' + message['role'] + '|]' + message['content'] }}{% if message['role'] == 'user' %}{{ '\n' }}{% else %}{{ '[|endofturn|]\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '[|assistant|]' }}{% endif %}
        ```

Or, you can use custom prompt as below.
<p>
<img src="assets/lmstudio_prompt.png", width="60%", style="margin: 20 auto;">

## Deployment

EXAONE Deep models have been integrated into various deployment frameworks. 

> [!Important]
> Before your deployment of EXAONE Deep models, we recommend following the [Usage Guideline](#usage-guideline) section to achieve the expected performance. 

### TensorRT-LLM

TensorRT-LLM has supported EXAONE language models since EXAONE 3.0. We recommend using TensorRT-LLM for the best performance. You can run EXAONE Deep models with TensorRT-LLM by following the instructions on [TensorRT-LLM EXAONE Example](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/models/core/exaone).

> [!Note]
> When you convert EXAONE Deep models to TensorRT-LLM format, you may need to set the environment variable `TRTLLM_DISABLE_UNIFIED_CONVERTER=1`.

> [!Note]
> TensorRT-LLM also supports AWQ on their own methods.
> If you want to use AWQ with TensorRT-LLM, please refer to the [AWQ section](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/models/core/exaone#groupwise-quantization-awq) in TensorRT-LLM EXAONE Example.

### vLLM

You can easily run EXAONE Deep models with vLLM. 

1. Install vLLM (`vllm>=0.6.0`). Please refer to the [vLLM quickstart guide](https://docs.vllm.ai/en/latest/getting_started/quickstart.html) for more details.

```bash
pip install vllm
```

2. Run the models with vLLM.

```bash
vllm serve LGAI-EXAONE/EXAONE-Deep-7.8B
```

3. Send a request with the following curl command after the server starts.

```bash
curl -X POST http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "LGAI-EXAONE/EXAONE-Deep-7.8B",
        "messages": [
            {"role": "user", "content": "How many golf balls can fit in a school bus?"}
        ],
        "max_tokens": 30720,
        "temperature": 0.6,
        "top_p": 0.95
    }'
```

> [!Note]
> If you want to serve GGUF quantized models with vLLM, please refer to the [vLLM GGUF documentation](https://docs.vllm.ai/en/latest/quantization/gguf.html).

### SGLang

You can also run EXAONE Deep models with SGLang.

1. Install SGLang. Please refer to the [SGLang documentation](https://sgl-project.github.io) for more details.

2. Run the server with the following command.

```bash
python -m sglang.launch_server --model-path LGAI-EXAONE/EXAONE-Deep-7.8B \
    --port 30000 --host 0.0.0.0
```

> [!Note]
> In case of using EXAONE Deep 2.4B model, you need to install sglang>=0.3.6 and use `--attention-backend triton` option.
>
> Additionally, we are currently working on a PR to fix the incompatibility of flashinfer with EXAONE Deep 2.4B. 
> Please refer to the [PR](https://github.com/sgl-project/sglang/pull/4064) for details.

3. Send a request with the following curl command after the server starts.

```bash
curl -s http://0.0.0.0:30000/v1/chat/completions \
    -d '{
        "model": "LGAI-EXAONE/EXAONE-Deep-7.8B",
        "messages": [
            {"role": "user", "content": "How many golf balls can fit in a school bus?"}
        ],
        "max_tokens": 30720,
        "temperature": 0.6,
        "top_p": 0.95,
    }'
```

<br>

## Usage Guideline

To achieve the expected performance, we recommend using the following configurations:

1. Ensure the model starts with `<thought>\n` for reasoning steps. The model's output quality may be degraded when you omit it. You can easily apply this feature by using `tokenizer.apply_chat_template()` with `add_generation_prompt=True`. Please check the example code on [Quickstart](#quickstart) section.
2. The reasoning steps of EXAONE Deep models enclosed by `<thought>\n...\n</thought>` usually have lots of tokens, so previous reasoning steps may be necessary to be removed in multi-turn situation. The provided tokenizer handles this automatically.
3. Avoid using system prompt, and build the instruction on the user prompt. 
4. Additional instructions help the models reason more deeply, so that the models generate better output.
    - For math problems, the instructions **"Please reason step by step, and put your final answer within \boxed{}."** are helpful.
    - For more information on our evaluation setting including prompts, please refer to our [Documentation](https://arxiv.org/abs/2503.12524).
5. In our evaluation, we use `temperature=0.6` and `top_p=0.95` for generation. 
6. When evaluating the models, it is recommended to test multiple times to assess the expected performance accurately.

<br>

## Limitation

The EXAONE language model has certain limitations and may occasionally generate inappropriate responses. The language model generates responses based on the output probability of tokens, and it is determined during learning from training data. While we have made every effort to exclude personal, harmful, and biased information from the training data, some problematic content may still be included, potentially leading to undesirable responses. Please note that the text generated by EXAONE language model does not reflects the views of LG AI Research.

- Inappropriate answers may be generated, which contain personal, harmful or other inappropriate information.
- Biased responses may be generated, which are associated with age, gender, race, and so on.
- The generated responses rely heavily on statistics from the training data, which can result in the generation of
semantically or syntactically incorrect sentences.
- Since the model does not reflect the latest information, the responses may be false or contradictory.

LG AI Research strives to reduce potential risks that may arise from EXAONE language models. Users are not allowed
to engage in any malicious activities (e.g., keying in illegal information) that may induce the creation of inappropriate
outputs violating LG AI's ethical principles when using EXAONE language models.

<br>

## License

The model is licensed under [EXAONE AI Model License Agreement 1.1 - NC](./LICENSE)
 
<br>
 
## Citation
 
```
@article{exaone-deep,
  title={EXAONE Deep: Reasoning Enhanced Language Models},
  author={{LG AI Research}},
  journal={arXiv preprint arXiv:2503.12524},
  year={2025}
}
```

<br>

## Contact
LG AI Research Technical Support: contact_us@lgresearch.ai
</file>

</files>
